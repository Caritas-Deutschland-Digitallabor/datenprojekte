{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230a6825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5174cd10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quelle</th>\n",
       "      <th>Projektname</th>\n",
       "      <th>Art</th>\n",
       "      <th>Einsatzbereich</th>\n",
       "      <th>Webseite-Link</th>\n",
       "      <th>Organisation</th>\n",
       "      <th>Ansprechperson</th>\n",
       "      <th>Email</th>\n",
       "      <th>Status</th>\n",
       "      <th>Kurzzusammenfassung</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Quelle, Projektname, Art, Einsatzbereich, Webseite-Link, Organisation, Ansprechperson, Email, Status, Kurzzusammenfassung, Unnamed: 10]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_PublicInterestAI = pd.read_csv(\"PublicInterestAI_Projekte.csv\", sep=\";\")\n",
    "df_PublicInterestAI.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462b4f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping all projects...\n",
      "Loading page...\n",
      "Scraping page 1...\n",
      "Found 40 project buttons on page 1\n",
      "No more pages found\n",
      "\n",
      "Total projects scraped: 40\n",
      "\n",
      "Sample projects with soup data:\n",
      "\n",
      "Project 1:\n",
      "Title: \"KIVI\"  KI + vigilareDüsseldorf, GermanyMedia Authority of NRW\n",
      "Number of target divs found: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Project 2:\n",
      "Title: ADISRotterdam, NetherlandsThe Ocean Cleanup Projects B.V.\n",
      "Number of target divs found: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Project 3:\n",
      "Title: AI4GridsKonstanz, GermanyHTWG-Konstanz\n",
      "Number of target divs found: 0\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Found 22 Germany-related projects\n"
     ]
    }
   ],
   "source": [
    "def scrape_all_projects():\n",
    "    \"\"\"\n",
    "    Scrape all projects by finding the specific button classes\n",
    "    \"\"\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading page...\")\n",
    "        driver.get(\"https://publicinterest.ai/tool/map/directory\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        all_projects = []\n",
    "        page_num = 1\n",
    "        \n",
    "        while True:\n",
    "            print(f\"Scraping page {page_num}...\")\n",
    "            \n",
    "            # Wait for content to load\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"button\")))\n",
    "            except:\n",
    "                print(\"No buttons found\")\n",
    "                break\n",
    "            \n",
    "            # Get page source\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Find the specific project buttons\n",
    "            project_buttons = soup.find_all('button', class_='sc-1b0661b3-0 sc-ef00fa6-0 cMVLjt bGXFPi')\n",
    "            \n",
    "            if not project_buttons:\n",
    "                # Try more flexible approach\n",
    "                project_buttons = soup.find_all('button', class_=lambda x: x and 'sc-1b0661b3-0' in x and 'sc-ef00fa6-0' in x)\n",
    "            \n",
    "            if not project_buttons:\n",
    "                # Even more flexible - just look for buttons with those class patterns\n",
    "                project_buttons = soup.find_all('button', class_=lambda x: x and 'cMVLjt' in x and 'bGXFPi' in x)\n",
    "            \n",
    "            print(f\"Found {len(project_buttons)} project buttons on page {page_num}\")\n",
    "            \n",
    "            if not project_buttons:\n",
    "                print(\"No project buttons found, stopping\")\n",
    "                break\n",
    "            \n",
    "            # Extract project data from each button\n",
    "            for button in project_buttons:\n",
    "                project_data = extract_project_data(button)\n",
    "                if project_data:\n",
    "                    all_projects.append(project_data)\n",
    "            \n",
    "            # Try to find next page button\n",
    "            try:\n",
    "                # Look for pagination or load more buttons\n",
    "                next_selectors = [\n",
    "                    'button[aria-label*=\"next\"]',\n",
    "                    'button[aria-label*=\"Next\"]', \n",
    "                    'button:contains(\"Load more\")',\n",
    "                    'button:contains(\"Show more\")',\n",
    "                    '.pagination button:last-child'\n",
    "                ]\n",
    "                \n",
    "                next_clicked = False\n",
    "                for selector in next_selectors:\n",
    "                    try:\n",
    "                        next_buttons = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        for btn in next_buttons:\n",
    "                            if btn.is_enabled() and btn.is_displayed():\n",
    "                                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                                time.sleep(3)\n",
    "                                next_clicked = True\n",
    "                                break\n",
    "                        if next_clicked:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not next_clicked:\n",
    "                    print(\"No more pages found\")\n",
    "                    break\n",
    "                    \n",
    "                page_num += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Pagination error: {e}\")\n",
    "                break\n",
    "        \n",
    "        return all_projects\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def extract_project_data(button):\n",
    "    \"\"\"\n",
    "    Extract data from project button, focusing on sc-4576c65c-1 colBIV divs\n",
    "    \"\"\"\n",
    "    project = {}\n",
    "    \n",
    "    try:\n",
    "        # Look for the specific div class you mentioned\n",
    "        target_divs = button.find_all('div', class_='sc-4576c65c-1 colBIV')\n",
    "        \n",
    "        if not target_divs:\n",
    "            # Try more flexible approach\n",
    "            target_divs = button.find_all('div', class_=lambda x: x and 'sc-4576c65c-1' in x)\n",
    "        \n",
    "        if not target_divs:\n",
    "            # Even more flexible\n",
    "            target_divs = button.find_all('div', class_=lambda x: x and 'colBIV' in x)\n",
    "        \n",
    "        # Extract the complete HTML soup for each target div\n",
    "        project['target_divs_soup'] = []\n",
    "        project['target_divs_html'] = []\n",
    "        \n",
    "        for i, div in enumerate(target_divs):\n",
    "            # Store the BeautifulSoup object (as string representation)\n",
    "            project['target_divs_soup'].append(str(div))\n",
    "            \n",
    "            # Store the HTML content\n",
    "            project['target_divs_html'].append(div.prettify())\n",
    "            \n",
    "            # Also extract structured data from each div\n",
    "            div_data = {\n",
    "                'index': i,\n",
    "                'classes': div.get('class', []),\n",
    "                'id': div.get('id', ''),\n",
    "                'text_content': div.get_text(strip=True),\n",
    "                'attributes': dict(div.attrs),\n",
    "                'child_elements': []\n",
    "            }\n",
    "            \n",
    "            # Get information about child elements\n",
    "            for child in div.find_all(recursive=False):  # Direct children only\n",
    "                child_info = {\n",
    "                    'tag': child.name,\n",
    "                    'classes': child.get('class', []),\n",
    "                    'text': child.get_text(strip=True),\n",
    "                    'attributes': dict(child.attrs)\n",
    "                }\n",
    "                div_data['child_elements'].append(child_info)\n",
    "            \n",
    "            project.setdefault('div_details', []).append(div_data)\n",
    "        \n",
    "        # Extract title (usually the first or largest text)\n",
    "        title = \"\"\n",
    "        for div in target_divs:\n",
    "            text = div.get_text(strip=True)\n",
    "            if text and len(text) > len(title):\n",
    "                title = text\n",
    "        \n",
    "        if not title:\n",
    "            # Fallback - get any prominent text from the button\n",
    "            all_text = button.get_text(strip=True).split('\\n')\n",
    "            for text in all_text:\n",
    "                if text and len(text) > 5 and len(text) < 200:\n",
    "                    title = text\n",
    "                    break\n",
    "        \n",
    "        project['title'] = title or \"No title found\"\n",
    "        \n",
    "        # Get text content from target divs\n",
    "        div_contents = []\n",
    "        for div in target_divs:\n",
    "            content = div.get_text(strip=True)\n",
    "            if content:\n",
    "                div_contents.append(content)\n",
    "        \n",
    "        project['div_text_contents'] = div_contents\n",
    "        \n",
    "        # Extract other useful data\n",
    "        project['button_soup'] = str(button)  # Complete button HTML\n",
    "        project['button_classes'] = ' '.join(button.get('class', []))\n",
    "        \n",
    "        # Get all links within the target divs\n",
    "        div_links = []\n",
    "        for div in target_divs:\n",
    "            for a in div.find_all('a'):\n",
    "                href = a.get('href')\n",
    "                if href:\n",
    "                    if href.startswith('/'):\n",
    "                        href = 'https://publicinterest.ai' + href\n",
    "                    div_links.append({\n",
    "                        'url': href,\n",
    "                        'text': a.get_text(strip=True),\n",
    "                        'attributes': dict(a.attrs)\n",
    "                    })\n",
    "        project['div_links'] = div_links\n",
    "        \n",
    "        # Extract any images within target divs\n",
    "        div_images = []\n",
    "        for div in target_divs:\n",
    "            for img in div.find_all('img'):\n",
    "                src = img.get('src', '')\n",
    "                if src.startswith('/'):\n",
    "                    src = 'https://publicinterest.ai' + src\n",
    "                div_images.append({\n",
    "                    'src': src,\n",
    "                    'alt': img.get('alt', ''),\n",
    "                    'attributes': dict(img.attrs)\n",
    "                })\n",
    "        project['div_images'] = div_images\n",
    "        \n",
    "        # Get complete text for filtering/searching later\n",
    "        project['full_text'] = button.get_text(separator=' | ', strip=True)\n",
    "        \n",
    "        return project\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting project: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    print(\"Scraping all projects...\")\n",
    "    projects = scrape_all_projects()\n",
    "    \n",
    "    print(f\"\\nTotal projects scraped: {len(projects)}\")\n",
    "    \n",
    "    if projects:\n",
    "        # Save to JSON\n",
    "        with open('all_projects.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(projects, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(\"\\nSample projects with soup data:\")\n",
    "        for i, project in enumerate(projects[:3]):  # Show fewer but with more detail\n",
    "            print(f\"\\nProject {i+1}:\")\n",
    "            print(f\"Title: {project.get('title', 'N/A')}\")\n",
    "            print(f\"Number of target divs found: {len(project.get('target_divs_soup', []))}\")\n",
    "            \n",
    "            # Show the HTML soup for each target div\n",
    "            for j, div_soup in enumerate(project.get('target_divs_soup', [])):\n",
    "                print(f\"\\nTarget Div {j+1} HTML:\")\n",
    "                print(div_soup)\n",
    "                print(f\"\\nTarget Div {j+1} Prettified:\")\n",
    "                if j < len(project.get('target_divs_html', [])):\n",
    "                    print(project['target_divs_html'][j])\n",
    "            \n",
    "            # Show structured div details\n",
    "            if project.get('div_details'):\n",
    "                print(f\"\\nDiv Details:\")\n",
    "                for detail in project['div_details']:\n",
    "                    print(f\"  Div {detail['index']}: classes={detail['classes']}, text='{detail['text_content'][:100]}...'\")\n",
    "                    print(f\"  Child elements: {len(detail['child_elements'])}\")\n",
    "                    for child in detail['child_elements']:\n",
    "                        print(f\"    {child['tag']}: {child['text'][:50]}...\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Filter for Germany-related projects after scraping\n",
    "        german_projects = []\n",
    "        search_terms = ['germany', 'german', 'deutschland', 'berlin', 'munich', 'hamburg']\n",
    "        \n",
    "        for project in projects:\n",
    "            full_text = project.get('full_text', '').lower()\n",
    "            if any(term in full_text for term in search_terms):\n",
    "                german_projects.append(project)\n",
    "        \n",
    "        print(f\"\\nFound {len(german_projects)} Germany-related projects\")\n",
    "        \n",
    "        if german_projects:\n",
    "            with open('german_projects.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(german_projects, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return projects\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    projects = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e502bebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<button class=\"sc-1b0661b3-0 sc-ef00fa6-0 cMVLjt bGXFPi\"><p>\"KIVI\"  KI + vigilare </p><ul class=\"sc-94b8f193-0 oMHBF\"><li class=\"sc-83595954-1 MTNHn\"><span class=\"undefined svg\" style='display: block; width: 100%; height: 100%; background-position: center center; background-repeat: no-repeat; background-size: contain; background-image: url(\"data:image/svg+xml,%3Csvg%20xmlns%3D\\\"http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg\\\"%20width%3D\\\"11.398\\\"%20height%3D\\\"16\\\"%3E%3Cpath%20fill%3D\\\"%23fff\\\"%20d%3D\\\"M5.699%200a5.7%205.7%200%200%200-4.567%209.109l3.655%206.332a1%201%200%200%200%20.068.119l.008.014a1.022%201.022%200%200%200%201.613.063l.01.006.036-.063a1%201%200%200%200%20.135-.234l3.582-6.2A5.7%205.7%200%200%200%205.699%200m-.056%208.589a2.808%202.808%200%201%201%202.809-2.808%202.81%202.81%200%200%201-2.808%202.808Z\\\"%2F%3E%3C%2Fsvg%3E\");'></span><span>Düsseldorf, Germany</span></li><li class=\"sc-83595954-1 MTNHn\"><span class=\"undefined svg\" style='display: block; width: 100%; height: 100%; background-position: center center; background-repeat: no-repeat; background-size: contain; background-image: url(\"data:image/svg+xml,%3Csvg%20xmlns%3D\\\"http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg\\\"%20width%3D\\\"12\\\"%20height%3D\\\"12\\\"%3E%3Cpath%20fill%3D\\\"%23fff\\\"%20d%3D\\\"M11.143%200H6a.86.86%200%200%200-.857.857v4.286H.857A.86.86%200%200%200%200%206v6h12V.857A.86.86%200%200%200%2011.143%200M3%2011.143v-3h1.714v3Zm8.143%200H5.571V7.714a.43.43%200%200%200-.429-.429H2.571a.43.43%200%200%200-.429.429v3.429H.857V6H6V.857h5.143Z\\\"%2F%3E%3C%2Fsvg%3E\");'></span><span>Media Authority of NRW</span></li></ul></button>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(projects[0][\"button_soup\"], 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16aef4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = soup.find_all('a')\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7043d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping all projects...\n",
      "Loading page...\n",
      "Scraping page 1...\n",
      "Found 40 clickable project buttons on page 1\n",
      "Processing button 1/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-44>:228: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing button 2/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 3/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 4/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 5/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 6/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 7/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 8/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 9/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 10/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 11/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 12/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 13/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 14/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 15/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 16/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 17/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 18/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 19/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 20/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 21/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 22/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 23/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 24/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 25/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 26/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 27/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 28/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 29/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 30/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 31/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 32/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 33/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 34/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 35/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 36/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 37/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 38/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 39/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "Processing button 40/40\n",
      "  Found 1 target divs (sc-4576c65c-0 ffKmeh)\n",
      "No more pages found\n",
      "\n",
      "Total projects scraped: 40\n",
      "\n",
      "Sample projects with clicked content:\n",
      "\n",
      "Project 1 (Button 0):\n",
      "Title: \"KIVI\"  KI + vigilare\n",
      "Initial divs found: 1\n",
      "Target divs found after clicking: 1\n",
      "\n",
      "--- Target Div 1 (sc-4576c65c-0 ffKmeh) ---\n",
      "Raw HTML:\n",
      "<div class=\"sc-4576c65c-0 ffKmeh\"><div class=\"sc-4576c65c-1 colBIV\"><div class=\"sc-4576c65c-2 dqWSyo\"><h1>\"KIVI\"  KI + vigilare </h1></div><div class=\"sc-94b8f193-2 sc-4576c65c-5 jAjQuf bKmOvb\"><ul class=\"sc-94b8f193-0 oMHBF\"><li class=\"sc-83595954-1 MTNHn\"><span class=\"undefined svg\" style='display: block; width: 100%; height: 100%; background-position: center center; background-repeat: no-repeat; background-size: contain; background-image: url(\"data:image/svg+xml,%3Csvg%20xmlns%3D\\\"http%3A%2F%...\n",
      "\n",
      "Text content:\n",
      "\"KIVI\"  KI + vigilareDüsseldorf, GermanyAugust 2020Media Authority of NRWOur goals: Supporting the monitoring process in its research on the internet which means an automatic research on the web for potential violations of the law and processing of automatically generated and pre-categorised finding...\n",
      "Links found: 1\n",
      "Images found: 0\n",
      "Text elements: 18\n",
      "  Link: View full project profile -> https://publicinterest.ai/tool/map/project/kivi-ki-vigilare\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Project 2 (Button 1):\n",
      "Title: Industrial Sectors:\n",
      "Initial divs found: 1\n",
      "Target divs found after clicking: 1\n",
      "\n",
      "--- Target Div 1 (sc-4576c65c-0 ffKmeh) ---\n",
      "Raw HTML:\n",
      "<div class=\"sc-4576c65c-0 ffKmeh\"><div class=\"sc-4576c65c-1 colBIV\"><div class=\"sc-4576c65c-2 dqWSyo\"><h1>ADIS</h1></div><div class=\"sc-94b8f193-2 sc-4576c65c-5 jAjQuf bKmOvb\"><ul class=\"sc-94b8f193-0 oMHBF\"><li class=\"sc-83595954-1 MTNHn\"><span class=\"undefined svg\" style='display: block; width: 100%; height: 100%; background-position: center center; background-repeat: no-repeat; background-size: contain; background-image: url(\"data:image/svg+xml,%3Csvg%20xmlns%3D\\\"http%3A%2F%2Fwww.w3.org%2F200...\n",
      "\n",
      "Text content:\n",
      "ADISRotterdam, NetherlandsJanuary 2021The Ocean Cleanup Projects B.V.ADIS is based on cameras mounted to vessels and artificial intelligence. In the exploratory stage of the project, we have collected over 10 million (20 TB) offshore images by outdoor, GPS-enabled action cameras. We developed an art...\n",
      "Links found: 1\n",
      "Images found: 0\n",
      "Text elements: 18\n",
      "  Link: View full project profile -> https://publicinterest.ai/tool/map/project/adis\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Project 3 (Button 2):\n",
      "Title: Industrial Sectors:\n",
      "Initial divs found: 1\n",
      "Target divs found after clicking: 1\n",
      "\n",
      "--- Target Div 1 (sc-4576c65c-0 ffKmeh) ---\n",
      "Raw HTML:\n",
      "<div class=\"sc-4576c65c-0 ffKmeh\"><div class=\"sc-4576c65c-1 colBIV\"><div class=\"sc-4576c65c-2 dqWSyo\"><h1>AI4Grids</h1></div><div class=\"sc-94b8f193-2 sc-4576c65c-5 jAjQuf bKmOvb\"><ul class=\"sc-94b8f193-0 oMHBF\"><li class=\"sc-83595954-1 MTNHn\"><span class=\"undefined svg\" style='display: block; width: 100%; height: 100%; background-position: center center; background-repeat: no-repeat; background-size: contain; background-image: url(\"data:image/svg+xml,%3Csvg%20xmlns%3D\\\"http%3A%2F%2Fwww.w3.org%2...\n",
      "\n",
      "Text content:\n",
      "AI4GridsKonstanz, GermanySeptember 2020HTWG-KonstanzThe aim of the project is to efficiently integrate the generators and consumers required for the energy transition into the medium- and low-voltage grid by means of intelligent grid management. In this way, a better synchronization of energy quanti...\n",
      "Links found: 1\n",
      "Images found: 0\n",
      "Text elements: 18\n",
      "  Link: View full project profile -> https://publicinterest.ai/tool/map/project/ai4grids\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Found 0 Germany-related projects\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "def scrape_all_projects():\n",
    "    \"\"\"\n",
    "    Scrape all projects by finding buttons, clicking them, and collecting detailed content\n",
    "    \"\"\"\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    \n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 15)\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading page...\")\n",
    "        driver.get(\"https://publicinterest.ai/tool/map/directory\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "        all_projects = []\n",
    "        page_num = 1\n",
    "        \n",
    "        while True:\n",
    "            print(f\"Scraping page {page_num}...\")\n",
    "            \n",
    "            # Wait for content to load\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"button\")))\n",
    "            except:\n",
    "                print(\"No buttons found\")\n",
    "                break\n",
    "            \n",
    "            # Find all project buttons using Selenium (for clicking)\n",
    "            selenium_buttons = driver.find_elements(By.CSS_SELECTOR, 'button.sc-1b0661b3-0.sc-ef00fa6-0.cMVLjt.bGXFPi')\n",
    "            \n",
    "            if not selenium_buttons:\n",
    "                # Try more flexible approach\n",
    "                selenium_buttons = driver.find_elements(By.CSS_SELECTOR, 'button[class*=\"sc-1b0661b3-0\"][class*=\"sc-ef00fa6-0\"]')\n",
    "            \n",
    "            if not selenium_buttons:\n",
    "                # Even more flexible\n",
    "                selenium_buttons = driver.find_elements(By.CSS_SELECTOR, 'button[class*=\"cMVLjt\"][class*=\"bGXFPi\"]')\n",
    "            \n",
    "            print(f\"Found {len(selenium_buttons)} clickable project buttons on page {page_num}\")\n",
    "            \n",
    "            if not selenium_buttons:\n",
    "                print(\"No project buttons found, stopping\")\n",
    "                break\n",
    "            \n",
    "            # Click each button and collect detailed data\n",
    "            for i, button in enumerate(selenium_buttons):\n",
    "                print(f\"Processing button {i+1}/{len(selenium_buttons)}\")\n",
    "                \n",
    "                try:\n",
    "                    # Scroll to button and click\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", button)\n",
    "                    time.sleep(1)\n",
    "                    \n",
    "                    # Click the button\n",
    "                    driver.execute_script(\"arguments[0].click();\", button)\n",
    "                    time.sleep(2)  # Wait for details to load\n",
    "                    \n",
    "                    # Get the page content after clicking\n",
    "                    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                    \n",
    "                    # Extract data from the clicked state\n",
    "                    project_data = extract_clicked_project_data(soup, i)\n",
    "                    \n",
    "                    if project_data:\n",
    "                        all_projects.append(project_data)\n",
    "                    \n",
    "                    # Close modal/details if there's a close button\n",
    "                    try:\n",
    "                        close_selectors = [\n",
    "                            'button[aria-label*=\"close\"]',\n",
    "                            'button[aria-label*=\"Close\"]',\n",
    "                            '.modal-close',\n",
    "                            '[data-testid*=\"close\"]',\n",
    "                            'button:contains(\"×\")',\n",
    "                            'button:contains(\"Close\")'\n",
    "                        ]\n",
    "                        \n",
    "                        for selector in close_selectors:\n",
    "                            try:\n",
    "                                close_btn = driver.find_element(By.CSS_SELECTOR, selector)\n",
    "                                if close_btn.is_displayed():\n",
    "                                    driver.execute_script(\"arguments[0].click();\", close_btn)\n",
    "                                    time.sleep(1)\n",
    "                                    break\n",
    "                            except:\n",
    "                                continue\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    # Press Escape key as fallback to close modal\n",
    "                    try:\n",
    "                        from selenium.webdriver.common.keys import Keys\n",
    "                        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)\n",
    "                        time.sleep(1)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing button {i+1}: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Try to find next page button\n",
    "            try:\n",
    "                next_selectors = [\n",
    "                    'button[aria-label*=\"next\"]',\n",
    "                    'button[aria-label*=\"Next\"]', \n",
    "                    'button:contains(\"Load more\")',\n",
    "                    'button:contains(\"Show more\")',\n",
    "                    '.pagination button:last-child'\n",
    "                ]\n",
    "                \n",
    "                next_clicked = False\n",
    "                for selector in next_selectors:\n",
    "                    try:\n",
    "                        next_buttons = driver.find_elements(By.CSS_SELECTOR, selector)\n",
    "                        for btn in next_buttons:\n",
    "                            if btn.is_enabled() and btn.is_displayed():\n",
    "                                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                                time.sleep(3)\n",
    "                                next_clicked = True\n",
    "                                break\n",
    "                        if next_clicked:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not next_clicked:\n",
    "                    print(\"No more pages found\")\n",
    "                    break\n",
    "                    \n",
    "                page_num += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Pagination error: {e}\")\n",
    "                break\n",
    "        \n",
    "        return all_projects\n",
    "        \n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def extract_clicked_project_data(soup, button_index):\n",
    "    \"\"\"\n",
    "    Extract data after clicking a project button, focusing on sc-4576c65c-0 ffKmeh divs\n",
    "    \"\"\"\n",
    "    project = {\n",
    "        'button_index': button_index,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Look for the initial project button divs (sc-4576c65c-1 colBIV)\n",
    "        initial_divs = soup.find_all('div', class_='sc-4576c65c-1 colBIV')\n",
    "        if not initial_divs:\n",
    "            initial_divs = soup.find_all('div', class_=lambda x: x and 'colBIV' in x)\n",
    "        \n",
    "        project['initial_divs_count'] = len(initial_divs)\n",
    "        project['initial_divs_soup'] = [str(div) for div in initial_divs]\n",
    "        \n",
    "        # Look for the target divs that appear after clicking (sc-4576c65c-0 ffKmeh)\n",
    "        target_divs = soup.find_all('div', class_='sc-4576c65c-0 ffKmeh')\n",
    "        \n",
    "        if not target_divs:\n",
    "            # Try more flexible approach\n",
    "            target_divs = soup.find_all('div', class_=lambda x: x and 'sc-4576c65c-0' in x)\n",
    "        \n",
    "        if not target_divs:\n",
    "            # Even more flexible\n",
    "            target_divs = soup.find_all('div', class_=lambda x: x and 'ffKmeh' in x)\n",
    "        \n",
    "        print(f\"  Found {len(target_divs)} target divs (sc-4576c65c-0 ffKmeh)\")\n",
    "        \n",
    "        # Extract complete soup and details for each target div\n",
    "        project['target_divs_soup'] = []\n",
    "        project['target_divs_html'] = []\n",
    "        project['target_divs_details'] = []\n",
    "        \n",
    "        for i, div in enumerate(target_divs):\n",
    "            # Store the complete HTML\n",
    "            project['target_divs_soup'].append(str(div))\n",
    "            project['target_divs_html'].append(div.prettify())\n",
    "            \n",
    "            # Extract structured details\n",
    "            div_detail = {\n",
    "                'index': i,\n",
    "                'classes': div.get('class', []),\n",
    "                'id': div.get('id', ''),\n",
    "                'text_content': div.get_text(strip=True),\n",
    "                'attributes': dict(div.attrs),\n",
    "                'all_links': [],\n",
    "                'all_images': [],\n",
    "                'all_text_elements': []\n",
    "            }\n",
    "            \n",
    "            # Extract all links within this div\n",
    "            for a in div.find_all('a'):\n",
    "                href = a.get('href', '')\n",
    "                if href.startswith('/'):\n",
    "                    href = 'https://publicinterest.ai' + href\n",
    "                div_detail['all_links'].append({\n",
    "                    'url': href,\n",
    "                    'text': a.get_text(strip=True),\n",
    "                    'attributes': dict(a.attrs)\n",
    "                })\n",
    "            \n",
    "            # Extract all images\n",
    "            for img in div.find_all('img'):\n",
    "                src = img.get('src', '')\n",
    "                if src.startswith('/'):\n",
    "                    src = 'https://publicinterest.ai' + src\n",
    "                div_detail['all_images'].append({\n",
    "                    'src': src,\n",
    "                    'alt': img.get('alt', ''),\n",
    "                    'attributes': dict(img.attrs)\n",
    "                })\n",
    "            \n",
    "            # Extract all text elements with their tags\n",
    "            for element in div.find_all(text=True):\n",
    "                parent = element.parent\n",
    "                if parent and element.strip():\n",
    "                    div_detail['all_text_elements'].append({\n",
    "                        'tag': parent.name,\n",
    "                        'text': element.strip(),\n",
    "                        'parent_classes': parent.get('class', [])\n",
    "                    })\n",
    "            \n",
    "            project['target_divs_details'].append(div_detail)\n",
    "        \n",
    "        # Extract title from various sources\n",
    "        title = \"\"\n",
    "        \n",
    "        # Try from target divs first\n",
    "        for div in target_divs:\n",
    "            headings = div.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "            for h in headings:\n",
    "                text = h.get_text(strip=True)\n",
    "                if text and len(text) > len(title):\n",
    "                    title = text\n",
    "        \n",
    "        # Fallback to initial divs\n",
    "        if not title:\n",
    "            for div in initial_divs:\n",
    "                text = div.get_text(strip=True)\n",
    "                if text and len(text) > len(title) and len(text) < 200:\n",
    "                    title = text\n",
    "        \n",
    "        project['title'] = title or f\"Project {button_index + 1}\"\n",
    "        \n",
    "        # Get all visible text from target divs\n",
    "        target_div_texts = []\n",
    "        for div in target_divs:\n",
    "            text = div.get_text(strip=True)\n",
    "            if text:\n",
    "                target_div_texts.append(text)\n",
    "        \n",
    "        project['target_div_texts'] = target_div_texts\n",
    "        project['target_divs_count'] = len(target_divs)\n",
    "        \n",
    "        # Look for any modal or popup content\n",
    "        modal_selectors = [\n",
    "            '[role=\"dialog\"]',\n",
    "            '.modal',\n",
    "            '[data-testid*=\"modal\"]',\n",
    "            '[aria-modal=\"true\"]'\n",
    "        ]\n",
    "        \n",
    "        modal_content = []\n",
    "        for selector in modal_selectors:\n",
    "            modals = soup.select(selector)\n",
    "            for modal in modals:\n",
    "                modal_content.append({\n",
    "                    'selector': selector,\n",
    "                    'html': str(modal),\n",
    "                    'text': modal.get_text(strip=True)\n",
    "                })\n",
    "        \n",
    "        project['modal_content'] = modal_content\n",
    "        \n",
    "        return project\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting clicked project data: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_project_data(button):\n",
    "    \"\"\"\n",
    "    Extract data from project button, focusing on sc-4576c65c-1 colBIV divs\n",
    "    \"\"\"\n",
    "    project = {}\n",
    "    \n",
    "    try:\n",
    "        # Look for the specific div class you mentioned\n",
    "        target_divs = button.find_all('div', class_='sc-4576c65c-1 colBIV')\n",
    "        \n",
    "        if not target_divs:\n",
    "            # Try more flexible approach\n",
    "            target_divs = button.find_all('div', class_=lambda x: x and 'sc-4576c65c-1' in x)\n",
    "        \n",
    "        if not target_divs:\n",
    "            # Even more flexible\n",
    "            target_divs = button.find_all('div', class_=lambda x: x and 'colBIV' in x)\n",
    "        \n",
    "        # Extract the complete HTML soup for each target div\n",
    "        project['target_divs_soup'] = []\n",
    "        project['target_divs_html'] = []\n",
    "        \n",
    "        for i, div in enumerate(target_divs):\n",
    "            # Store the BeautifulSoup object (as string representation)\n",
    "            project['target_divs_soup'].append(str(div))\n",
    "            \n",
    "            # Store the HTML content\n",
    "            project['target_divs_html'].append(div.prettify())\n",
    "            \n",
    "            # Also extract structured data from each div\n",
    "            div_data = {\n",
    "                'index': i,\n",
    "                'classes': div.get('class', []),\n",
    "                'id': div.get('id', ''),\n",
    "                'text_content': div.get_text(strip=True),\n",
    "                'attributes': dict(div.attrs),\n",
    "                'child_elements': []\n",
    "            }\n",
    "            \n",
    "            # Get information about child elements\n",
    "            for child in div.find_all(recursive=False):  # Direct children only\n",
    "                child_info = {\n",
    "                    'tag': child.name,\n",
    "                    'classes': child.get('class', []),\n",
    "                    'text': child.get_text(strip=True),\n",
    "                    'attributes': dict(child.attrs)\n",
    "                }\n",
    "                div_data['child_elements'].append(child_info)\n",
    "            \n",
    "            project.setdefault('div_details', []).append(div_data)\n",
    "        \n",
    "        # Extract title (usually the first or largest text)\n",
    "        title = \"\"\n",
    "        for div in target_divs:\n",
    "            text = div.get_text(strip=True)\n",
    "            if text and len(text) > len(title):\n",
    "                title = text\n",
    "        \n",
    "        if not title:\n",
    "            # Fallback - get any prominent text from the button\n",
    "            all_text = button.get_text(strip=True).split('\\n')\n",
    "            for text in all_text:\n",
    "                if text and len(text) > 5 and len(text) < 200:\n",
    "                    title = text\n",
    "                    break\n",
    "        \n",
    "        project['title'] = title or \"No title found\"\n",
    "        \n",
    "        # Get text content from target divs\n",
    "        div_contents = []\n",
    "        for div in target_divs:\n",
    "            content = div.get_text(strip=True)\n",
    "            if content:\n",
    "                div_contents.append(content)\n",
    "        \n",
    "        project['div_text_contents'] = div_contents\n",
    "        \n",
    "        # Extract other useful data\n",
    "        project['button_soup'] = str(button)  # Complete button HTML\n",
    "        project['button_classes'] = ' '.join(button.get('class', []))\n",
    "        \n",
    "        # Get all links within the target divs\n",
    "        div_links = []\n",
    "        for div in target_divs:\n",
    "            for a in div.find_all('a'):\n",
    "                href = a.get('href')\n",
    "                if href:\n",
    "                    if href.startswith('/'):\n",
    "                        href = 'https://publicinterest.ai' + href\n",
    "                    div_links.append({\n",
    "                        'url': href,\n",
    "                        'text': a.get_text(strip=True),\n",
    "                        'attributes': dict(a.attrs)\n",
    "                    })\n",
    "        project['div_links'] = div_links\n",
    "        \n",
    "        # Extract any images within target divs\n",
    "        div_images = []\n",
    "        for div in target_divs:\n",
    "            for img in div.find_all('img'):\n",
    "                src = img.get('src', '')\n",
    "                if src.startswith('/'):\n",
    "                    src = 'https://publicinterest.ai' + src\n",
    "                div_images.append({\n",
    "                    'src': src,\n",
    "                    'alt': img.get('alt', ''),\n",
    "                    'attributes': dict(img.attrs)\n",
    "                })\n",
    "        project['div_images'] = div_images\n",
    "        \n",
    "        # Get complete text for filtering/searching later\n",
    "        project['full_text'] = button.get_text(separator=' | ', strip=True)\n",
    "        \n",
    "        return project\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting project: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    print(\"Scraping all projects...\")\n",
    "    projects = scrape_all_projects()\n",
    "    \n",
    "    print(f\"\\nTotal projects scraped: {len(projects)}\")\n",
    "    \n",
    "    if projects:\n",
    "        # Save to JSON\n",
    "        with open('all_projects.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(projects, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(\"\\nSample projects with clicked content:\")\n",
    "        for i, project in enumerate(projects[:3]):  # Show fewer but with more detail\n",
    "            print(f\"\\nProject {i+1} (Button {project.get('button_index', 'unknown')}):\")\n",
    "            print(f\"Title: {project.get('title', 'N/A')}\")\n",
    "            print(f\"Initial divs found: {project.get('initial_divs_count', 0)}\")\n",
    "            print(f\"Target divs found after clicking: {project.get('target_divs_count', 0)}\")\n",
    "            \n",
    "            # Show the HTML soup for each target div (sc-4576c65c-0 ffKmeh)\n",
    "            for j, div_soup in enumerate(project.get('target_divs_soup', [])):\n",
    "                print(f\"\\n--- Target Div {j+1} (sc-4576c65c-0 ffKmeh) ---\")\n",
    "                print(\"Raw HTML:\")\n",
    "                print(div_soup[:500] + \"...\" if len(div_soup) > 500 else div_soup)\n",
    "                \n",
    "                print(f\"\\nText content:\")\n",
    "                if j < len(project.get('target_div_texts', [])):\n",
    "                    print(project['target_div_texts'][j][:300] + \"...\" if len(project['target_div_texts'][j]) > 300 else project['target_div_texts'][j])\n",
    "                \n",
    "                # Show structured details\n",
    "                if j < len(project.get('target_divs_details', [])):\n",
    "                    details = project['target_divs_details'][j]\n",
    "                    print(f\"Links found: {len(details.get('all_links', []))}\")\n",
    "                    print(f\"Images found: {len(details.get('all_images', []))}\")\n",
    "                    print(f\"Text elements: {len(details.get('all_text_elements', []))}\")\n",
    "                    \n",
    "                    # Show links\n",
    "                    for link in details.get('all_links', [])[:3]:  # Show first 3 links\n",
    "                        print(f\"  Link: {link['text']} -> {link['url']}\")\n",
    "            \n",
    "            # Show modal content if any\n",
    "            if project.get('modal_content'):\n",
    "                print(f\"\\nModal content found: {len(project['modal_content'])} modals\")\n",
    "                for modal in project['modal_content']:\n",
    "                    print(f\"  Modal text preview: {modal['text'][:200]}...\")\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "        \n",
    "        # Filter for Germany-related projects after scraping\n",
    "        german_projects = []\n",
    "        search_terms = ['germany', 'german', 'deutschland', 'berlin', 'munich', 'hamburg']\n",
    "        \n",
    "        for project in projects:\n",
    "            full_text = project.get('full_text', '').lower()\n",
    "            if any(term in full_text for term in search_terms):\n",
    "                german_projects.append(project)\n",
    "        \n",
    "        print(f\"\\nFound {len(german_projects)} Germany-related projects\")\n",
    "        \n",
    "        if german_projects:\n",
    "            with open('german_projects.json', 'w', encoding='utf-8') as f:\n",
    "                json.dump(german_projects, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    return projects\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    projects = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4ba76f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_links = []\n",
    "\n",
    "for i in range(len(projects)):\n",
    "    project_links.append(projects[i][\"target_divs_details\"][0][\"all_links\"][0][\"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ce97f09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://publicinterest.ai/tool/map/project/kivi-ki-vigilare'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "86c78350",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_PublicInterestAI[\"Quelle\"] = project_links\n",
    "df_PublicInterestAI.to_csv(\"PublicInterestAI_Projekte.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
